{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train.csv\")\n",
    "test=pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "#### Data Exploration and Preparation\n",
    "\n",
    "You’ve received a new dataset for your upcoming project. Before diving into the main analysis, it's essential to assess the dataset’s consistency and prepare it for reliable and accurate results. Your task is to explore the data thoroughly and identify any potential issues that could impact your findings.\n",
    "\n",
    "Evaluate the completeness and consistency of the data. Look for anomalies, any values or patterns that appear unusual or unexpected. Use appropriate visualizations to highlight these irregularities effectively.\n",
    "\n",
    "Determine how to address these issues and clearly justify your approach. Your goal is to prepare a clean, reliable dataset that you can confidently use for analysis and modeling.\n",
    "\n",
    "Data preprocessing is a critical phase—ensure your work is meticulous and well-documented, as it will serve as the foundation for all future tasks.\n",
    "\n",
    "(1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here, along with reasoning\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Your code here, along with reasoning\n",
    "# Missing values (count & %)\n",
    "missing = train.isnull().sum()\n",
    "missing_pct = (missing/len(train))*100\n",
    "missing_df = pd.DataFrame({\"missing_count\": missing, \"missing_pct\": missing_pct})\n",
    "missing_df = missing_df.sort_values(\"missing_pct\", ascending=False)\n",
    "\n",
    "print(\"=== Missing Value Summary (Top 20) ===\")\n",
    "display(missing_df.head(20))\n",
    "\n",
    "# Plot missing percentages\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.barplot(x=missing_df.head(20).index, y=\"missing_pct\", data=missing_df.head(20), color=\"steelblue\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"% Missing\")\n",
    "plt.title(\"Top Missing Values (%)\")\n",
    "plt.show()\n",
    "\n",
    "# Duplicate rows\n",
    "dup_count = train.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows in training set: {dup_count}\")\n",
    "\n",
    "# Separate numeric and categorical columns (exclude obvious IDs)\n",
    "id_like = [c for c in train.columns if train[c].nunique() == len(train)]\n",
    "num_cols = train.select_dtypes(include=np.number).columns.difference(id_like)\n",
    "\n",
    "# Descriptive stats to spot unusual ranges or zeros\n",
    "desc = train[num_cols].describe().T\n",
    "display(desc)\n",
    "\n",
    "# Boxplots & histograms for first few numeric columns\n",
    "for col in num_cols[:6]:\n",
    "    fig, ax = plt.subplots(1,2, figsize=(10,3))\n",
    "    sns.histplot(train[col].dropna(), ax=ax[0], kde=True)\n",
    "    ax[0].set_title(f\"{col} - Histogram\")\n",
    "    sns.boxplot(x=train[col], ax=ax[1])\n",
    "    ax[1].set_title(f\"{col} - Boxplot\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Outlier counts using IQR method\n",
    "outlier_counts = {}\n",
    "for col in num_cols:\n",
    "    q1, q3 = train[col].quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    low, high = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "    outlier_counts[col] = ((train[col] < low) | (train[col] > high)).sum()\n",
    "print(\"\\nTop columns by outlier count:\")\n",
    "display(pd.Series(outlier_counts).sort_values(ascending=False).head(10))\n",
    "\n",
    "# Categorical columns (exclude IDs)\n",
    "cat_cols = train.select_dtypes(exclude=np.number).columns.difference(id_like)\n",
    "\n",
    "# Value counts for first few categorical columns\n",
    "for col in cat_cols[:5]:\n",
    "    print(f\"\\n--- {col} ---\")\n",
    "    print(train[col].value_counts(dropna=False).head(10))\n",
    "\n",
    "# Detect high-cardinality categorical columns\n",
    "high_card = [c for c in cat_cols if train[c].nunique() > 50]\n",
    "print(\"\\nHigh-cardinality categorical columns (>50 unique values):\", high_card)\n",
    "\n",
    "#Completeness: Reports and plots missing values, checks duplicates.\n",
    "#Numeric anomalies: Shows descriptive stats, histograms, and boxplots; flags columns with many outliers.\n",
    "#Categorical anomalies: Prints top frequencies to find unexpected categories; \n",
    "#lists high-cardinality columns that may need special encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "#### Data Driven Decisions and Analysis\n",
    "\n",
    "Your company wants to adopt a Generative AI-Tool to speed up and automate certain tasks. Your task now is to: \n",
    "- Conduct a thorough exploration of the relationships between all relevant variables in the dataset.\n",
    "- Develop **TWO** additional metrics which can provide some interesting insights into the success of the Gen AI tools in various sectors. Explain your findings. (Hint: Try to quantify how valuable the investment in Gen AI is)\n",
    "- Utilize appropriate visualization techniques to represent your findings.\n",
    "\n",
    "As you analyze the data try doing the following to understand the data better:\n",
    "- What overall trends and anomalies do you notice when examining the relationships between different variables?\n",
    "- Consider all variables and their effects in collaboration while making your decision.\n",
    "- Think of multiple data backed use cases that your company can pursue.\n",
    "\n",
    "\n",
    "Remember, the goal is to provide actionable insights that can inform data-driven decision-making at your company.\n",
    "\n",
    "(1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here, along with reasoning\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ===============================\n",
    "# 1. Explore relationships (correlation heatmap)\n",
    "# ===============================\n",
    "num_cols = ['productivity_change_pct','employees_impacted','new_roles_created',\n",
    "            'training_hours','deployment_cost','employee_sentiment_score',\n",
    "            'automation_coverage_pct','voluntary_attritions','genai_role_salary',\n",
    "            'Client_Satisfaction_Post_GenAI']\n",
    "\n",
    "corr = train[num_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(corr, cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Correlation Heatmap (GenAI Adoption Metrics)\", fontsize=14)\n",
    "plt.xticks(range(len(corr)), corr.columns, rotation=90)\n",
    "plt.yticks(range(len(corr)), corr.columns)\n",
    "plt.show()\n",
    "\n",
    "print(\"Top correlations:\\n\", \n",
    "      corr.unstack().sort_values(ascending=False).drop_duplicates().head(10))\n",
    "\n",
    "# ===============================\n",
    "# 2. Industry-level exploration\n",
    "# ===============================\n",
    "plt.figure(figsize=(10,5))\n",
    "train[\"industry\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Industry Distribution of GenAI Adoption\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Average productivity change by industry\n",
    "industry_perf = train.groupby(\"industry\")[\"productivity_change_pct\"].mean().sort_values(ascending=False)\n",
    "industry_perf.plot(kind=\"bar\", figsize=(10,5))\n",
    "plt.title(\"Average Productivity Change by Industry (Post GenAI)\")\n",
    "plt.ylabel(\"Mean % Change\")\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# 3. Define TWO New Business Metrics\n",
    "# ===============================\n",
    "\n",
    "# Metric 1: ROI Proxy (Productivity gain per dollar spent)\n",
    "train[\"ROI_Proxy\"] = train[\"productivity_change_pct\"] / train[\"deployment_cost\"]\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "train[\"ROI_Proxy\"].hist(bins=30)\n",
    "plt.title(\"Distribution of ROI Proxy (Productivity % per $ spent)\")\n",
    "plt.xlabel(\"ROI Proxy\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Metric 2: Employee Value Add (Client Satisfaction * Productivity) normalized by attrition\n",
    "train[\"Employee_Value_Add\"] = (\n",
    "    (train[\"Client_Satisfaction_Post_GenAI\"] * train[\"productivity_change_pct\"]) /\n",
    "    (train[\"voluntary_attritions\"] + 1)   # +1 to avoid divide by zero\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "train[\"Employee_Value_Add\"].plot(kind=\"box\")\n",
    "plt.title(\"Employee Value Add Score Distribution\")\n",
    "plt.ylabel(\"Value Add Score\")\n",
    "plt.show()\n",
    "\n",
    "# Summary of new metrics\n",
    "print(\"\\n=== New Metrics Summary ===\")\n",
    "print(train[[\"ROI_Proxy\",\"Employee_Value_Add\"]].describe())\n",
    "\n",
    "# ===============================\n",
    "# 4. Insights & Use Cases (Template)\n",
    "# ===============================\n",
    "print(\"\"\"\n",
    "Insights:\n",
    "1. Certain industries show the largest productivity gains from GenAI adoption.\n",
    "2. ROI_Proxy highlights which sectors achieve higher productivity improvements per dollar spent.\n",
    "3. Employee_Value_Add identifies where customer satisfaction and productivity improvements remain strong despite attrition.\n",
    "4. Correlations suggest that higher training_hours often align with better employee_sentiment_score, \n",
    "   meaning workforce enablement is critical.\n",
    "\n",
    "Use Cases:\n",
    "- Prioritize scaling GenAI in industries with the highest ROI_Proxy.\n",
    "- Use Employee_Value_Add to target sectors where GenAI improves both workforce outcomes and client satisfaction.\n",
    "- Invest in employee training programs, as they positively influence both sentiment and adoption success.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "#### Correlation Analysis\n",
    "\n",
    "1. Based on your correlation analysis, what strategies could organizations adopt to boost productivity and reduce employee impact? Are there any relationships in the data that seem unusual or unexpectedly strong?\n",
    "\n",
    "2. Could transforming or engineering any variables (such as normalizing sentiment scores, scaling deployment costs, or introducing ratios) help improve the clarity of their relationships with the target variables? What would your rationale be for applying such adjustments?\n",
    "Consider using Tukey’s Bulging Rule to explore whether certain non-linear relationships could be straightened using transformations (e.g. log, square root, reciprocal). Which variables might benefit from such transformations, and why?\n",
    "\n",
    "3. Plot an updated correlation matrix after implementing the adjustment(s). Based on this revised plot, what actionable steps can organizations take to improve the overall effectiveness of their GenAI adoption?\n",
    "\n",
    "(1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here, along with reasoning\n",
    "# --- Copy numeric columns ---\n",
    "numeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "df_corr = train[numeric_cols].copy()\n",
    "\n",
    "# Transformations (log for skewed vars)\n",
    "df_trans = df_corr.copy()\n",
    "for col in ['deployment_cost','genai_role_salary','employees_impacted']:\n",
    "    df_trans[col+'_log'] = np.log1p(df_trans[col])  # add transformed columns\n",
    "\n",
    "# Updated correlation matrix\n",
    "corr_matrix = df_trans.corr()\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\n",
    "plt.title(\"Updated Correlation Matrix After Transformations\")\n",
    "plt.show()\n",
    "\n",
    "# Safely plot scatter pairs: align by dropping NaNs on both variables together\n",
    "pairs = [\n",
    "    ('deployment_cost_log','productivity_change_pct'),\n",
    "    ('employees_impacted_log','productivity_change_pct'),\n",
    "    ('employee_sentiment_score','employees_impacted')\n",
    "]\n",
    "\n",
    "for x,y in pairs:\n",
    "    if x in df_trans.columns and y in df_trans.columns:\n",
    "        # Drop rows where either x or y is NaN\n",
    "        aligned = df_trans[[x,y]].dropna()\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.scatter(aligned[x], aligned[y], s=6)\n",
    "        plt.xlabel(x)\n",
    "        plt.ylabel(y)\n",
    "        plt.title(f'{x} vs {y}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "#### Model Building\n",
    "\n",
    "The primary objective is to develop a robust regression model capable of predicting two key target variables: `employees_impacted` and `productivity_change_pct`, in order to better understand and anticipate shifts in hiring dynamics across the workforce.\n",
    "\n",
    "You are tasked with constructing a regression-based model that effectively captures the relationship between a variety of input features and the aforementioned targets. Evaluate and experiment with different regression techniques, and provide a rationale for both the model and feature selection strategy you adopt.\n",
    "\n",
    "In your approach, consider the use of statistical measures such as Mallows’ Cp for feature selection. Additionally, explore and discuss the applicability of other selection metrics (e.g. AIC, BIC, adjusted R², cross-validation techniques), noting their strengths, limitations, and appropriate use cases. (**You need not write code for this part of the question**)\n",
    "\n",
    "(2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# Task 4: Model Building (Clean Version with Imports)\n",
    "# --------------------------------\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Features & targets\n",
    "X = train.drop(columns=[\"employees_impacted\", \"productivity_change_pct\"])\n",
    "y1 = train[\"employees_impacted\"].fillna(0)  # Fill NaN in targets\n",
    "y2 = train[\"productivity_change_pct\"].fillna(0)\n",
    "\n",
    "# Identify numeric & categorical columns\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "categorical_features = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "# Pipelines for numeric & categorical data with imputation\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "# Preprocessors\n",
    "preprocessor_y1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor_y2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Lasso\": Lasso(alpha=0.01, max_iter=10000),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_models(X, y, preprocessor, target_name):\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        pipe = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", model)])\n",
    "        cv_scores = cross_val_score(pipe, X, y, cv=5, scoring=\"r2\")\n",
    "        results.append({\n",
    "            \"Model\": name,\n",
    "            \"Target\": target_name,\n",
    "            \"Mean R²\": cv_scores.mean(),\n",
    "            \"Std R²\": cv_scores.std()\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run evaluations\n",
    "results_y1 = evaluate_models(X, y1, preprocessor_y1, \"employees_impacted\")\n",
    "results_y2 = evaluate_models(X, y2, preprocessor_y2, \"productivity_change_pct\")\n",
    "\n",
    "# Combine & show results\n",
    "results_all = pd.concat([results_y1, results_y2], ignore_index=True)\n",
    "results_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "\n",
    "#### Model Evaluation\n",
    "\n",
    "To ensure the reliability and effectiveness of your regression model, you need to select an appropriate evaluation metric to assess the models performance.\n",
    "\n",
    "Coefficient of Determination (R²) is a popular metric for regressive models. It is a measure of how well the model explains the variance in the target variable(s). Use R² to evaluate your model’s performance in predicting `employees_impacted` and `productivity_change_pct`.\n",
    "\n",
    "Can you think of possible drawbacks to simply evaluating explainability of variance?\n",
    "Explore other evaluation metrics suitable for your model and compare how they contribute to your understanding of the model’s quality.\n",
    "\n",
    "(1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here, along with reasoning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['company_name']=test['company_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just an example to illustrate how the submission works\n",
    "# Do not include this line of code in your actual submissions\n",
    "sample_submission=pd.read_csv(\"/kaggle/input/orange_1/sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['productivity_change_pct']=sample_submission['productivity_change_pct']\n",
    "submission['employees_impacted']=sample_submission['employees_impacted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations ! 🎉\n",
    "\n",
    "You've successfully completed the Data Analytics (UE23CS342AA2) Hackathon-1 assignment. This is a very significant milestone in your journey of Data Analytics.\n",
    "\n",
    "By completing this assignment, you have:\n",
    "\n",
    "- Applied foundational Data Analytics concepts and skills to real-world problems.\n",
    "- Built and experimented with various regression models.\n",
    "\n",
    "The knowledge and skills you’ve gained here form the bedrock of this exciting and ever-evolving field. Remember, this is just the beginning — stay curious, keep exploring, and continue learning!\n",
    "\n",
    "Wishing you the very best for your upcoming assignments and ISA-1."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13709654,
     "sourceId": 114817,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
